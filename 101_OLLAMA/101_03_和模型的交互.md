
# 1 模型的交互

以 gemma 模型为例



1 
 输入  `ollama run gemma:2b`
 会弹出 prompt  : 输入你想要的效果 
![](images/Pasted%20image%2020241105200804.png)

效果是 让他写一个故事 


2  不弹出 prompt 
直接输入 `ollama run gemma:2b  "please write a email"`


3  在输入的内容只用一些本地的文件 
![](images/Pasted%20image%2020241105201055.png)

4 --verbose 参数 

在执行后 输出系结构 , 给出一些 执行的整体参数 

![](images/Pasted%20image%2020241105201242.png)




# 2 用RestAPI 的访问 Ollama 

Ollama 继承了 OpenAI API 的接口 

11434 是 Ollama 的端口 


## 2.1 用curl 

![](images/Pasted%20image%2020241105201413.png)

model 的名称可以自己修改 

```bash
curl -X POST http://localhost:11434/api/generate -d '{"model":"my_model","prompt":"Hello, world!"}'
```


----



嘉定提问的人是 user 这个身份的 

![](images/Pasted%20image%2020241105215126.png)


----

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama2-chinese:13b",
  "messages": [
    {
      "role": "system",
      "content": "以海盗的口吻简单作答。"
    },
    {
      "role": "user",
      "content": "天空为什么是蓝色的？"
    }
  ],
  "stream": false
}'
```

其中 `role` 为 `system` 的消息即为系统提示词。


---

## 2.2 用python 

![](images/Pasted%20image%2020241105201547.png)



![](images/Pasted%20image%2020241105201630.png)




