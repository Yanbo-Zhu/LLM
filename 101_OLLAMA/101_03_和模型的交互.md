
# 1 直接提供提示词

以 gemma 模型为例

1 
 输入  `ollama run gemma:2b`
 会弹出 prompt  : 输入你想要的效果 
![](images/Pasted%20image%2020241105200804.png)

效果是 让他写一个故事 


2  不弹出 prompt 
直接输入 `ollama run gemma:2b  "please write a email"`


3  在输入的内容只用一些本地的文件 
![](images/Pasted%20image%2020241105201055.png)

"$(cat READ.md)"

4 --verbose 参数 

在执行后 输出系结构 , 给出一些 执行的整体参数 

![](images/Pasted%20image%2020241105201242.png)




# 2 用RestAPI 的访问 Ollama 


## 2.1 启动端口 

![](images/Pasted%20image%2020241106125933.png)

Ollama 继承了 OpenAI API 的接口 
11434 是 Ollama 的端口 


## 2.2 用curl 

https://github.com/ollama/ollama/blob/main/docs/api.md

```bash
curl -X POST http://localhost:11434/api/generate -d '{"model":"my_model","prompt":"Hello, world!"}'
```

prompt: 用户想要input给 llm 的值 
steam:  if `false` the response will be returned as a single response object, rather than a stream of objects
options :   additional model parameters listed in the documentation for the [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values) such as `temperature`



### 2.2.1 例子 


![](images/Pasted%20image%2020241106130318.png)

```
curl http://localhost:11434/api/generate -d '{ 
    "model": "gemma:2b",
    "prompt": "你好",
    "stream": false,
    "options": { 
        "temperature": 0.5
    }
}' | jq '.response, model'
```


----

1 
![](images/Pasted%20image%2020241105201413.png)
model 的名称可以自己修改 



2 
假定提问的人是 user 这个身份的 

![](images/Pasted%20image%2020241105215126.png)


3 
```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama2-chinese:13b",
  "messages": [
    {
      "role": "system",
      "content": "以海盗的口吻简单作答。"
    },
    {
      "role": "user",
      "content": "天空为什么是蓝色的？"
    }
  ],
  "stream": false
}'
```

其中 `role` 为 `system` 的消息即为系统提示词。


## 2.3 用python 

![](images/Pasted%20image%2020241105201547.png)



![](images/Pasted%20image%2020241105201630.png)




